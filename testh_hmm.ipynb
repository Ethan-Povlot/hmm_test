{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r hmmlearn -t ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util.py \n",
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "\n",
    "def normalize(a, axis=None):\n",
    "    a_sum = a.sum(axis)\n",
    "    if axis and a.ndim > 1:\n",
    "        a_sum[a_sum == 0] = 1\n",
    "        shape = list(a.shape)\n",
    "        shape[axis] = 1\n",
    "        a_sum.shape = shape\n",
    "\n",
    "    a /= a_sum\n",
    "\n",
    "\n",
    "def log_normalize(a, axis=None):\n",
    "    if axis is not None and a.shape[axis] == 1:\n",
    "        a[:] = 0\n",
    "    else:\n",
    "        with np.errstate(under=\"ignore\"):\n",
    "            a_lse = special.logsumexp(a, axis, keepdims=True)\n",
    "        a -= a_lse\n",
    "\n",
    "\n",
    "def fill_covars(covars, covariance_type='full', n_components=1, n_features=1):\n",
    "    if covariance_type == 'full':\n",
    "        return covars\n",
    "    elif covariance_type == 'diag':\n",
    "        return np.array(list(map(np.diag, covars)))\n",
    "    elif covariance_type == 'tied':\n",
    "        return np.tile(covars, (n_components, 1, 1))\n",
    "    elif covariance_type == 'spherical':\n",
    "        covars = np.ravel(covars)\n",
    "        eye = np.eye(n_features)[np.newaxis, :, :]\n",
    "        covars = covars[:, np.newaxis, np.newaxis]\n",
    "        return eye * covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base.py\n",
    "\n",
    "import logging\n",
    "import string\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg, special\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import (\n",
    "    check_array, check_is_fitted, check_random_state)\n",
    "\n",
    "from . import _hmmc, _kl_divergence as _kl, _utils\n",
    "from .utils import normalize, log_normalize\n",
    "\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "#: Supported decoder algorithms.\n",
    "DECODER_ALGORITHMS = frozenset((\"viterbi\", \"map\"))\n",
    "\n",
    "\n",
    "class ConvergenceMonitor:\n",
    "\n",
    "\n",
    "    _template = \"{iter:>10d} {log_prob:>16.8f} {delta:>+16.8f}\"\n",
    "\n",
    "    def __init__(self, tol, n_iter, verbose):\n",
    "        \n",
    "        self.tol = tol\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.history = deque()\n",
    "        self.iter = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        params = sorted(dict(vars(self), history=list(self.history)).items())\n",
    "        return (\"{}(\\n\".format(class_name)\n",
    "                + \"\".join(map(\"    {}={},\\n\".format, *zip(*params)))\n",
    "                + \")\")\n",
    "\n",
    "    def _reset(self):\n",
    "        self.iter = 0\n",
    "        self.history.clear()\n",
    "\n",
    "    def report(self, log_prob):\n",
    "        \n",
    "        if self.verbose:\n",
    "            delta = log_prob - self.history[-1] if self.history else np.nan\n",
    "            message = self._template.format(\n",
    "                iter=self.iter + 1, log_prob=log_prob, delta=delta)\n",
    "            print(message, file=sys.stderr)\n",
    "\n",
    "        # Allow for some wiggleroom based on precision.\n",
    "        precision = np.finfo(float).eps ** (1/2)\n",
    "        if self.history and (log_prob - self.history[-1]) < -precision:\n",
    "            delta = log_prob - self.history[-1]\n",
    "            _log.warning(f\"Model is not converging.  Current: {log_prob}\"\n",
    "                         f\" is not greater than {self.history[-1]}.\"\n",
    "                         f\" Delta is {delta}\")\n",
    "        self.history.append(log_prob)\n",
    "        self.iter += 1\n",
    "\n",
    "    @property\n",
    "    def converged(self):\n",
    "        \"\"\"Whether the EM algorithm converged.\"\"\"\n",
    "        # XXX we might want to check that ``log_prob`` is non-decreasing.\n",
    "        return (self.iter == self.n_iter or\n",
    "                (len(self.history) >= 2 and\n",
    "                 self.history[-1] - self.history[-2] < self.tol))\n",
    "\n",
    "\n",
    "class _AbstractHMM(BaseEstimator):\n",
    "\n",
    "    def __init__(self, n_components, algorithm, random_state, n_iter,\n",
    "                 tol, verbose, params, init_params, implementation):\n",
    "       \n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.params = params\n",
    "        self.init_params = init_params\n",
    "        self.algorithm = algorithm\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.implementation = implementation\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def score_samples(self, X, lengths=None):\n",
    "        return self._score(X, lengths, compute_posteriors=True)\n",
    "\n",
    "    def score(self, X, lengths=None):\n",
    "        return self._score(X, lengths, compute_posteriors=False)[0]\n",
    "\n",
    "    def _score(self, X, lengths=None, *, compute_posteriors):\n",
    "        check_is_fitted(self, \"startprob_\")\n",
    "        self._check()\n",
    "\n",
    "        X = check_array(X)\n",
    "        impl = {\n",
    "            \"scaling\": self._score_scaling,\n",
    "            \"log\": self._score_log,\n",
    "        }[self.implementation]\n",
    "        return impl(\n",
    "            X=X, lengths=lengths, compute_posteriors=compute_posteriors)\n",
    "\n",
    "    def _score_log(self, X, lengths=None, *, compute_posteriors):\n",
    "        log_prob = 0\n",
    "        sub_posteriors = [np.empty((0, self.n_components))]\n",
    "        for sub_X in _utils.split_X_lengths(X, lengths):\n",
    "            log_frameprob = self._compute_log_likelihood(sub_X)\n",
    "            log_probij, fwdlattice = _hmmc.forward_log(\n",
    "                self.startprob_, self.transmat_, log_frameprob)\n",
    "            log_prob += log_probij\n",
    "            if compute_posteriors:\n",
    "                bwdlattice = _hmmc.backward_log(\n",
    "                    self.startprob_, self.transmat_, log_frameprob)\n",
    "                sub_posteriors.append(\n",
    "                    self._compute_posteriors_log(fwdlattice, bwdlattice))\n",
    "        return log_prob, np.concatenate(sub_posteriors)\n",
    "\n",
    "    def _score_scaling(self, X, lengths=None, *, compute_posteriors):\n",
    "        log_prob = 0\n",
    "        sub_posteriors = [np.empty((0, self.n_components))]\n",
    "        for sub_X in _utils.split_X_lengths(X, lengths):\n",
    "            frameprob = self._compute_likelihood(sub_X)\n",
    "            log_probij, fwdlattice, scaling_factors = _hmmc.forward_scaling(\n",
    "                self.startprob_, self.transmat_, frameprob)\n",
    "            log_prob += log_probij\n",
    "            if compute_posteriors:\n",
    "                bwdlattice = _hmmc.backward_scaling(\n",
    "                    self.startprob_, self.transmat_,\n",
    "                    frameprob, scaling_factors)\n",
    "                sub_posteriors.append(\n",
    "                    self._compute_posteriors_scaling(fwdlattice, bwdlattice))\n",
    "\n",
    "        return log_prob, np.concatenate(sub_posteriors)\n",
    "\n",
    "    def _decode_viterbi(self, X):\n",
    "        log_frameprob = self._compute_log_likelihood(X)\n",
    "        return _hmmc.viterbi(self.startprob_, self.transmat_, log_frameprob)\n",
    "\n",
    "    def _decode_map(self, X):\n",
    "        _, posteriors = self.score_samples(X)\n",
    "        log_prob = np.max(posteriors, axis=1).sum()\n",
    "        state_sequence = np.argmax(posteriors, axis=1)\n",
    "        return log_prob, state_sequence\n",
    "\n",
    "    def decode(self, X, lengths=None, algorithm=None):\n",
    "        check_is_fitted(self, \"startprob_\")\n",
    "        self._check()\n",
    "\n",
    "        algorithm = algorithm or self.algorithm\n",
    "        if algorithm not in DECODER_ALGORITHMS:\n",
    "            raise ValueError(f\"Unknown decoder {algorithm!r}\")\n",
    "\n",
    "        decoder = {\n",
    "            \"viterbi\": self._decode_viterbi,\n",
    "            \"map\": self._decode_map\n",
    "        }[algorithm]\n",
    "\n",
    "        X = check_array(X)\n",
    "        log_prob = 0\n",
    "        sub_state_sequences = []\n",
    "        for sub_X in _utils.split_X_lengths(X, lengths):\n",
    "            # XXX decoder works on a single sample at a time!\n",
    "            sub_log_prob, sub_state_sequence = decoder(sub_X)\n",
    "            log_prob += sub_log_prob\n",
    "            sub_state_sequences.append(sub_state_sequence)\n",
    "\n",
    "        return log_prob, np.concatenate(sub_state_sequences)\n",
    "\n",
    "    def predict(self, X, lengths=None):\n",
    "        _, state_sequence = self.decode(X, lengths)\n",
    "        return state_sequence\n",
    "\n",
    "    def predict_proba(self, X, lengths=None):\n",
    "        _, posteriors = self.score_samples(X, lengths)\n",
    "        return posteriors\n",
    "\n",
    "    def sample(self, n_samples=1, random_state=None, currstate=None):\n",
    "        check_is_fitted(self, \"startprob_\")\n",
    "        self._check()\n",
    "\n",
    "        if random_state is None:\n",
    "            random_state = self.random_state\n",
    "        random_state = check_random_state(random_state)\n",
    "\n",
    "        transmat_cdf = np.cumsum(self.transmat_, axis=1)\n",
    "\n",
    "        if currstate is None:\n",
    "            startprob_cdf = np.cumsum(self.startprob_)\n",
    "            currstate = (startprob_cdf > random_state.rand()).argmax()\n",
    "\n",
    "        state_sequence = [currstate]\n",
    "        X = [self._generate_sample_from_state(\n",
    "            currstate, random_state=random_state)]\n",
    "\n",
    "        for t in range(n_samples - 1):\n",
    "            currstate = (\n",
    "                (transmat_cdf[currstate] > random_state.rand()).argmax())\n",
    "            state_sequence.append(currstate)\n",
    "            X.append(self._generate_sample_from_state(\n",
    "                currstate, random_state=random_state))\n",
    "\n",
    "        return np.atleast_2d(X), np.array(state_sequence, dtype=int)\n",
    "\n",
    "    \n",
    "\n",
    "    def _fit_scaling(self, X):\n",
    "        raise NotImplementedError(\"Must be overridden in subclass\")\n",
    "\n",
    "    def _fit_log(self, X):\n",
    "        raise NotImplementedError(\"Must be overridden in subclass\")\n",
    "\n",
    "    def _compute_posteriors_scaling(self, fwdlattice, bwdlattice):\n",
    "        posteriors = fwdlattice * bwdlattice\n",
    "        normalize(posteriors, axis=1)\n",
    "        return posteriors\n",
    "\n",
    "    def _compute_posteriors_log(self, fwdlattice, bwdlattice):\n",
    "        log_gamma = fwdlattice + bwdlattice\n",
    "        log_normalize(log_gamma, axis=1)\n",
    "        with np.errstate(under=\"ignore\"):\n",
    "            return np.exp(log_gamma)\n",
    "\n",
    "    def _needs_init(self, code, name):\n",
    "        if code in self.init_params:\n",
    "            if hasattr(self, name):\n",
    "                _log.warning(\n",
    "                    \"Even though the %r attribute is set, it will be \"\n",
    "                    \"overwritten during initialization because 'init_params' \"\n",
    "                    \"contains %r\", name, code)\n",
    "            return True\n",
    "        if not hasattr(self, name):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _check_and_set_n_features(self, X):\n",
    "        _, n_features = X.shape\n",
    "        if hasattr(self, \"n_features\"):\n",
    "            if self.n_features != n_features:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected number of dimensions, got {n_features} but \"\n",
    "                    f\"expected {self.n_features}\")\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "\n",
    "    def _get_n_fit_scalars_per_param(self):\n",
    "        raise NotImplementedError(\"Must be overridden in subclass\")\n",
    "\n",
    "    def _check_sum_1(self, name):\n",
    "        \"\"\"Check that an array describes one or more distributions.\"\"\"\n",
    "        s = getattr(self, name).sum(axis=-1)\n",
    "        if not np.allclose(s, 1):\n",
    "            raise ValueError(\n",
    "                f\"{name} must sum to 1 (got {s:.4f})\" if s.ndim == 0 else\n",
    "                f\"{name} rows must sum to 1 (got {s})\" if s.ndim == 1 else\n",
    "                \"Expected 1D or 2D array\")\n",
    "\n",
    "    def _check(self):\n",
    "        raise NotImplementedError(\"Must be overridden in subclass\")\n",
    "\n",
    "    def _compute_likelihood(self, X):\n",
    "        if (self._compute_log_likelihood  # prevent recursion\n",
    "                != __class__._compute_log_likelihood.__get__(self)):\n",
    "            # Probabilities equal to zero do occur, and exp(-LARGE) = 0 is OK.\n",
    "            with np.errstate(under=\"ignore\"):\n",
    "                return np.exp(self._compute_log_likelihood(X))\n",
    "        else:\n",
    "            raise NotImplementedError(\"Must be overridden in subclass\")\n",
    "\n",
    "    def _compute_log_likelihood(self, X):\n",
    "        if (self._compute_likelihood  # prevent recursion\n",
    "                != __class__._compute_likelihood.__get__(self)):\n",
    "            # Probabilities equal to zero do occur, and log(0) = -inf is OK.\n",
    "            likelihood = self._compute_likelihood(X)\n",
    "            with np.errstate(divide=\"ignore\"):\n",
    "                return np.log(likelihood)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Must be overridden in subclass\")\n",
    "\n",
    "    def _generate_sample_from_state(self, state, random_state):\n",
    "        return ()\n",
    "\n",
    "    def _initialize_sufficient_statistics(self):\n",
    "        stats = {'nobs': 0,\n",
    "                 'start': np.zeros(self.n_components),\n",
    "                 'trans': np.zeros((self.n_components, self.n_components))}\n",
    "        return stats\n",
    "\n",
    "    def _accumulate_sufficient_statistics(\n",
    "            self, stats, X, lattice, posteriors, fwdlattice, bwdlattice):\n",
    "        impl = {\n",
    "            \"scaling\": self._accumulate_sufficient_statistics_scaling,\n",
    "            \"log\": self._accumulate_sufficient_statistics_log,\n",
    "        }[self.implementation]\n",
    "\n",
    "        return impl(stats=stats, X=X, lattice=lattice, posteriors=posteriors,\n",
    "                    fwdlattice=fwdlattice, bwdlattice=bwdlattice)\n",
    "\n",
    "    def _accumulate_sufficient_statistics_scaling(\n",
    "            self, stats, X, lattice, posteriors, fwdlattice, bwdlattice):\n",
    "        stats['nobs'] += 1\n",
    "        if 's' in self.params:\n",
    "            stats['start'] += posteriors[0]\n",
    "        if 't' in self.params:\n",
    "            n_samples, n_components = lattice.shape\n",
    "            # when the sample is of length 1, it contains no transitions\n",
    "            # so there is no reason to update our trans. matrix estimate\n",
    "            if n_samples <= 1:\n",
    "                return\n",
    "            xi_sum = _hmmc.compute_scaling_xi_sum(\n",
    "                fwdlattice, self.transmat_, bwdlattice, lattice)\n",
    "            stats['trans'] += xi_sum\n",
    "\n",
    "    def _accumulate_sufficient_statistics_log(\n",
    "            self, stats, X, lattice, posteriors, fwdlattice, bwdlattice):\n",
    "        stats['nobs'] += 1\n",
    "        if 's' in self.params:\n",
    "            stats['start'] += posteriors[0]\n",
    "        if 't' in self.params:\n",
    "            n_samples, n_components = lattice.shape\n",
    "            # when the sample is of length 1, it contains no transitions\n",
    "            # so there is no reason to update our trans. matrix estimate\n",
    "            if n_samples <= 1:\n",
    "                return\n",
    "            log_xi_sum = _hmmc.compute_log_xi_sum(\n",
    "                fwdlattice, self.transmat_, bwdlattice, lattice)\n",
    "            with np.errstate(under=\"ignore\"):\n",
    "                stats['trans'] += np.exp(log_xi_sum)\n",
    "\n",
    "\n",
    "\n",
    "    def _do_estep(self, X, lengths):\n",
    "        impl = {\n",
    "            \"scaling\": self._fit_scaling,\n",
    "            \"log\": self._fit_log,\n",
    "        }[self.implementation]\n",
    "\n",
    "        stats = self._initialize_sufficient_statistics()\n",
    "        self._estep_begin()\n",
    "        curr_logprob = 0\n",
    "        for sub_X in _utils.split_X_lengths(X, lengths):\n",
    "            lattice, logprob, posteriors, fwdlattice, bwdlattice = impl(sub_X)\n",
    "            # Derived HMM classes will implement the following method to\n",
    "            # update their probability distributions, so keep\n",
    "            # a single call to this method for simplicity.\n",
    "            self._accumulate_sufficient_statistics(\n",
    "                stats, sub_X, lattice, posteriors, fwdlattice,\n",
    "                bwdlattice)\n",
    "            curr_logprob += logprob\n",
    "        return stats, curr_logprob\n",
    "\n",
    "    def _estep_begin(self):\n",
    "        pass\n",
    "\n",
    "    def _compute_lower_bound(self, curr_logprob):\n",
    "        raise NotImplementedError(\"Must be overridden in subclass\")\n",
    "\n",
    "\n",
    "class BaseHMM(_AbstractHMM):\n",
    "    \n",
    "    def __init__(self, n_components=1,\n",
    "                 startprob_prior=1.0, transmat_prior=1.0,\n",
    "                 algorithm=\"viterbi\", random_state=None,\n",
    "                 n_iter=10, tol=1e-2, verbose=False,\n",
    "                 params=string.ascii_letters,\n",
    "                 init_params=string.ascii_letters,\n",
    "                 implementation=\"log\"):\n",
    "\n",
    "        super().__init__(\n",
    "            n_components=n_components, algorithm=algorithm,\n",
    "            random_state=random_state, n_iter=n_iter, tol=tol,\n",
    "            verbose=verbose, params=params, init_params=init_params,\n",
    "            implementation=implementation)\n",
    "        self.startprob_prior = startprob_prior\n",
    "        self.transmat_prior = transmat_prior\n",
    "        self.monitor_ = ConvergenceMonitor(self.tol, self.n_iter, self.verbose)\n",
    "\n",
    "    def get_stationary_distribution(self):\n",
    "        \"\"\"Compute the stationary distribution of states.\"\"\"\n",
    "        # The stationary distribution is proportional to the left-eigenvector\n",
    "        # associated with the largest eigenvalue (i.e., 1) of the transition\n",
    "        # matrix.\n",
    "        check_is_fitted(self, \"transmat_\")\n",
    "        eigvals, eigvecs = linalg.eig(self.transmat_.T)\n",
    "        eigvec = np.real_if_close(eigvecs[:, np.argmax(eigvals)])\n",
    "        return eigvec / eigvec.sum()\n",
    "\n",
    "    def _fit_scaling(self, X):\n",
    "        frameprob = self._compute_likelihood(X)\n",
    "        log_prob, fwdlattice, scaling_factors = _hmmc.forward_scaling(\n",
    "            self.startprob_, self.transmat_, frameprob)\n",
    "        bwdlattice = _hmmc.backward_scaling(\n",
    "            self.startprob_, self.transmat_, frameprob, scaling_factors)\n",
    "        posteriors = self._compute_posteriors_scaling(fwdlattice, bwdlattice)\n",
    "        return frameprob, log_prob, posteriors, fwdlattice, bwdlattice\n",
    "\n",
    "    def _fit_log(self, X):\n",
    "        log_frameprob = self._compute_log_likelihood(X)\n",
    "        log_prob, fwdlattice = _hmmc.forward_log(\n",
    "            self.startprob_, self.transmat_, log_frameprob)\n",
    "        bwdlattice = _hmmc.backward_log(\n",
    "            self.startprob_, self.transmat_, log_frameprob)\n",
    "        posteriors = self._compute_posteriors_log(fwdlattice, bwdlattice)\n",
    "        return log_frameprob, log_prob, posteriors, fwdlattice, bwdlattice\n",
    "\n",
    "    def _do_mstep(self, stats):\n",
    "        if 's' in self.params:\n",
    "            startprob_ = np.maximum(self.startprob_prior - 1 + stats['start'],\n",
    "                                    0)\n",
    "            self.startprob_ = np.where(self.startprob_ == 0, 0, startprob_)\n",
    "            normalize(self.startprob_)\n",
    "        if 't' in self.params:\n",
    "            transmat_ = np.maximum(self.transmat_prior - 1 + stats['trans'], 0)\n",
    "            self.transmat_ = np.where(self.transmat_ == 0, 0, transmat_)\n",
    "            normalize(self.transmat_, axis=1)\n",
    "\n",
    "    def _compute_lower_bound(self, curr_logprob):\n",
    "        return curr_logprob\n",
    "\n",
    "    def _init(self, X, lengths=None):\n",
    "        self._check_and_set_n_features(X)\n",
    "        init = 1. / self.n_components\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        if self._needs_init(\"s\", \"startprob_\"):\n",
    "            self.startprob_ = random_state.dirichlet(\n",
    "                np.full(self.n_components, init))\n",
    "        if self._needs_init(\"t\", \"transmat_\"):\n",
    "            self.transmat_ = random_state.dirichlet(\n",
    "                np.full(self.n_components, init), size=self.n_components)\n",
    "        n_fit_scalars_per_param = self._get_n_fit_scalars_per_param()\n",
    "        if n_fit_scalars_per_param is not None:\n",
    "            n_fit_scalars = sum(\n",
    "                n_fit_scalars_per_param[p] for p in self.params)\n",
    "            if X.size < n_fit_scalars:\n",
    "                _log.warning(\n",
    "                    \"Fitting a model with %d free scalar parameters with only \"\n",
    "                    \"%d data points will result in a degenerate solution.\",\n",
    "                    n_fit_scalars, X.size)\n",
    "\n",
    "    def _check_sum_1(self, name):\n",
    "        \"\"\"Check that an array describes one or more distributions.\"\"\"\n",
    "        s = getattr(self, name).sum(axis=-1)\n",
    "        if not np.allclose(s, 1):\n",
    "            raise ValueError(\n",
    "                f\"{name} must sum to 1 (got {s:.4f})\" if s.ndim == 0 else\n",
    "                f\"{name} rows must sum to 1 (got {s})\" if s.ndim == 1 else\n",
    "                \"Expected 1D or 2D array\")\n",
    "\n",
    "    def _check(self):\n",
    "        self.startprob_ = np.asarray(self.startprob_)\n",
    "        if len(self.startprob_) != self.n_components:\n",
    "            raise ValueError(\"startprob_ must have length n_components\")\n",
    "        self._check_sum_1(\"startprob_\")\n",
    "\n",
    "        self.transmat_ = np.asarray(self.transmat_)\n",
    "        if self.transmat_.shape != (self.n_components, self.n_components):\n",
    "            raise ValueError(\n",
    "                \"transmat_ must have shape (n_components, n_components)\")\n",
    "        self._check_sum_1(\"transmat_\")\n",
    "\n",
    "    def aic(self, X, lengths=None):\n",
    "        n_params = sum(self._get_n_fit_scalars_per_param().values())\n",
    "        return -2 * self.score(X, lengths=lengths) + 2 * n_params\n",
    "\n",
    "    def bic(self, X, lengths=None):\n",
    "        n_params = sum(self._get_n_fit_scalars_per_param().values())\n",
    "        return -2 * self.score(X, lengths=lengths) + n_params * np.log(len(X))\n",
    "\n",
    "_BaseHMM = BaseHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from sklearn import cluster\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from . import _emissions, _utils\n",
    "from .base import BaseHMM\n",
    "from .utils import fill_covars, normalize\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"GMMHMM\", \"GaussianHMM\", \"CategoricalHMM\", \"MultinomialHMM\", \"PoissonHMM\",\n",
    "]\n",
    "\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "COVARIANCE_TYPES = frozenset((\"spherical\", \"diag\", \"full\", \"tied\"))\n",
    "\n",
    "\n",
    "class GMMHMM(_emissions.BaseGMMHMM):\n",
    "    def __init__(self, n_components=1, n_mix=1,\n",
    "                 min_covar=1e-3, startprob_prior=1.0, transmat_prior=1.0,\n",
    "                 weights_prior=1.0, means_prior=0.0, means_weight=0.0,\n",
    "                 covars_prior=None, covars_weight=None,\n",
    "                 algorithm=\"viterbi\", covariance_type=\"diag\",\n",
    "                 random_state=None, n_iter=10, tol=1e-2,\n",
    "                 verbose=False, params=\"stmcw\",\n",
    "                 init_params=\"stmcw\",\n",
    "                 implementation=\"log\"):\n",
    "\n",
    "        BaseHMM.__init__(self, n_components,\n",
    "                         startprob_prior=startprob_prior,\n",
    "                         transmat_prior=transmat_prior,\n",
    "                         algorithm=algorithm, random_state=random_state,\n",
    "                         n_iter=n_iter, tol=tol, verbose=verbose,\n",
    "                         params=params, init_params=init_params,\n",
    "                         implementation=implementation)\n",
    "        self.covariance_type = covariance_type\n",
    "        self.min_covar = min_covar\n",
    "        self.n_mix = n_mix\n",
    "        self.weights_prior = weights_prior\n",
    "        self.means_prior = means_prior\n",
    "        self.means_weight = means_weight\n",
    "        self.covars_prior = covars_prior\n",
    "        self.covars_weight = covars_weight\n",
    "\n",
    "    def _init(self, X, lengths=None):\n",
    "        super()._init(X, lengths=None)\n",
    "        nc = self.n_components\n",
    "        nf = self.n_features\n",
    "        nm = self.n_mix\n",
    "\n",
    "        def compute_cv():\n",
    "            return np.cov(X.T) + self.min_covar * np.eye(nf)\n",
    "\n",
    "        # Default values for covariance prior parameters\n",
    "        self._init_covar_priors()\n",
    "        self._fix_priors_shape()\n",
    "\n",
    "        main_kmeans = cluster.KMeans(n_clusters=nc,\n",
    "                                     random_state=self.random_state,\n",
    "                                     n_init=10)  # sklearn >=1.2 compat.\n",
    "        cv = None  # covariance matrix\n",
    "        labels = main_kmeans.fit_predict(X)\n",
    "        main_centroid = np.mean(main_kmeans.cluster_centers_, axis=0)\n",
    "        means = []\n",
    "        for label in range(nc):\n",
    "            kmeans = cluster.KMeans(n_clusters=nm,\n",
    "                                    random_state=self.random_state,\n",
    "                                    n_init=10)  # sklearn >=1.2 compat.\n",
    "            X_cluster = X[np.where(labels == label)]\n",
    "            if X_cluster.shape[0] >= nm:\n",
    "                kmeans.fit(X_cluster)\n",
    "                means.append(kmeans.cluster_centers_)\n",
    "            else:\n",
    "                if cv is None:\n",
    "                    cv = compute_cv()\n",
    "                m_cluster = np.random.multivariate_normal(main_centroid,\n",
    "                                                          cov=cv,\n",
    "                                                          size=nm)\n",
    "                means.append(m_cluster)\n",
    "\n",
    "        if self._needs_init(\"w\", \"weights_\"):\n",
    "            self.weights_ = np.full((nc, nm), 1 / nm)\n",
    "\n",
    "        if self._needs_init(\"m\", \"means_\"):\n",
    "            self.means_ = np.stack(means)\n",
    "\n",
    "        if self._needs_init(\"c\", \"covars_\"):\n",
    "            if cv is None:\n",
    "                cv = compute_cv()\n",
    "            if not cv.shape:\n",
    "                cv.shape = (1, 1)\n",
    "            if self.covariance_type == 'tied':\n",
    "                self.covars_ = np.zeros((nc, nf, nf))\n",
    "                self.covars_[:] = cv\n",
    "            elif self.covariance_type == 'full':\n",
    "                self.covars_ = np.zeros((nc, nm, nf, nf))\n",
    "                self.covars_[:] = cv\n",
    "            elif self.covariance_type == 'diag':\n",
    "                self.covars_ = np.zeros((nc, nm, nf))\n",
    "                self.covars_[:] = np.diag(cv)\n",
    "            elif self.covariance_type == 'spherical':\n",
    "                self.covars_ = np.zeros((nc, nm))\n",
    "                self.covars_[:] = cv.mean()\n",
    "\n",
    "    def _init_covar_priors(self):\n",
    "        if self.covariance_type == \"full\":\n",
    "            if self.covars_prior is None:\n",
    "                self.covars_prior = 0.0\n",
    "            if self.covars_weight is None:\n",
    "                self.covars_weight = -(1.0 + self.n_features + 1.0)\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            if self.covars_prior is None:\n",
    "                self.covars_prior = 0.0\n",
    "            if self.covars_weight is None:\n",
    "                self.covars_weight = -(self.n_mix + self.n_features + 1.0)\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            if self.covars_prior is None:\n",
    "                self.covars_prior = -1.5\n",
    "            if self.covars_weight is None:\n",
    "                self.covars_weight = 0.0\n",
    "        elif self.covariance_type == \"spherical\":\n",
    "            if self.covars_prior is None:\n",
    "                self.covars_prior = -(self.n_mix + 2.0) / 2.0\n",
    "            if self.covars_weight is None:\n",
    "                self.covars_weight = 0.0\n",
    "\n",
    "    def _fix_priors_shape(self):\n",
    "        nc = self.n_components\n",
    "        nf = self.n_features\n",
    "        nm = self.n_mix\n",
    "        self.weights_prior = np.broadcast_to(\n",
    "            self.weights_prior, (nc, nm)).copy()\n",
    "        self.means_prior = np.broadcast_to(\n",
    "            self.means_prior, (nc, nm, nf)).copy()\n",
    "        self.means_weight = np.broadcast_to(\n",
    "            self.means_weight, (nc, nm)).copy()\n",
    "\n",
    "        if self.covariance_type == \"full\":\n",
    "            self.covars_prior = np.broadcast_to(\n",
    "                self.covars_prior, (nc, nm, nf, nf)).copy()\n",
    "            self.covars_weight = np.broadcast_to(\n",
    "                self.covars_weight, (nc, nm)).copy()\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            self.covars_prior = np.broadcast_to(\n",
    "                self.covars_prior, (nc, nf, nf)).copy()\n",
    "            self.covars_weight = np.broadcast_to(\n",
    "                self.covars_weight, nc).copy()\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            self.covars_prior = np.broadcast_to(\n",
    "                self.covars_prior, (nc, nm, nf)).copy()\n",
    "            self.covars_weight = np.broadcast_to(\n",
    "                self.covars_weight, (nc, nm, nf)).copy()\n",
    "        elif self.covariance_type == \"spherical\":\n",
    "            self.covars_prior = np.broadcast_to(\n",
    "                self.covars_prior, (nc, nm)).copy()\n",
    "            self.covars_weight = np.broadcast_to(\n",
    "                self.covars_weight, (nc, nm)).copy()\n",
    "\n",
    "    def _check(self):\n",
    "        super()._check()\n",
    "        if not hasattr(self, \"n_features\"):\n",
    "            self.n_features = self.means_.shape[2]\n",
    "        nc = self.n_components\n",
    "        nf = self.n_features\n",
    "        nm = self.n_mix\n",
    "\n",
    "        self._init_covar_priors()\n",
    "        self._fix_priors_shape()\n",
    "\n",
    "        # Checking covariance type\n",
    "        if self.covariance_type not in COVARIANCE_TYPES:\n",
    "            raise ValueError(\n",
    "                f\"covariance_type must be one of {COVARIANCE_TYPES}\")\n",
    "\n",
    "        self.weights_ = np.array(self.weights_)\n",
    "        # Checking mixture weights' shape\n",
    "        if self.weights_.shape != (nc, nm):\n",
    "            raise ValueError(\n",
    "                f\"weights_ must have shape (n_components, n_mix), \"\n",
    "                f\"actual shape: {self.weights_.shape}\")\n",
    "\n",
    "        # Checking mixture weights' mathematical correctness\n",
    "        self._check_sum_1(\"weights_\")\n",
    "\n",
    "        # Checking means' shape\n",
    "        self.means_ = np.array(self.means_)\n",
    "        if self.means_.shape != (nc, nm, nf):\n",
    "            raise ValueError(\n",
    "                f\"means_ must have shape (n_components, n_mix, n_features), \"\n",
    "                f\"actual shape: {self.means_.shape}\")\n",
    "\n",
    "        # Checking covariances' shape\n",
    "        self.covars_ = np.array(self.covars_)\n",
    "        covars_shape = self.covars_.shape\n",
    "        needed_shapes = {\n",
    "            \"spherical\": (nc, nm),\n",
    "            \"tied\": (nc, nf, nf),\n",
    "            \"diag\": (nc, nm, nf),\n",
    "            \"full\": (nc, nm, nf, nf),\n",
    "        }\n",
    "        needed_shape = needed_shapes[self.covariance_type]\n",
    "        if covars_shape != needed_shape:\n",
    "            raise ValueError(\n",
    "                f\"{self.covariance_type!r} mixture covars must have shape \"\n",
    "                f\"{needed_shape}, actual shape: {covars_shape}\")\n",
    "\n",
    "        # Checking covariances' mathematical correctness\n",
    "        if (self.covariance_type == \"spherical\" or\n",
    "                self.covariance_type == \"diag\"):\n",
    "            if np.any(self.covars_ < 0):\n",
    "                raise ValueError(f\"{self.covariance_type!r} mixture covars \"\n",
    "                                 f\"must be non-negative\")\n",
    "            if np.any(self.covars_ == 0):\n",
    "                _log.warning(\"Degenerate mixture covariance\")\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            for i, covar in enumerate(self.covars_):\n",
    "                if not np.allclose(covar, covar.T):\n",
    "                    raise ValueError(\n",
    "                        f\"Covariance of state #{i} is not symmetric\")\n",
    "                min_eigvalsh = linalg.eigvalsh(covar).min()\n",
    "                if min_eigvalsh < 0:\n",
    "                    raise ValueError(\n",
    "                        f\"Covariance of state #{i} is not positive definite\")\n",
    "                if min_eigvalsh == 0:\n",
    "                    _log.warning(\"Covariance of state #%d has a null \"\n",
    "                                 \"eigenvalue.\", i)\n",
    "        elif self.covariance_type == \"full\":\n",
    "            for i, mix_covars in enumerate(self.covars_):\n",
    "                for j, covar in enumerate(mix_covars):\n",
    "                    if not np.allclose(covar, covar.T):\n",
    "                        raise ValueError(\n",
    "                            f\"Covariance of state #{i}, mixture #{j} is not \"\n",
    "                            f\"symmetric\")\n",
    "                    min_eigvalsh = linalg.eigvalsh(covar).min()\n",
    "                    if min_eigvalsh < 0:\n",
    "                        raise ValueError(\n",
    "                            f\"Covariance of state #{i}, mixture #{j} is not \"\n",
    "                            f\"positive definite\")\n",
    "                    if min_eigvalsh == 0:\n",
    "                        _log.warning(\"Covariance of state #%d, mixture #%d \"\n",
    "                                     \"has a null eigenvalue.\", i, j)\n",
    "\n",
    "    def _do_mstep(self, stats):\n",
    "        super()._do_mstep(stats)\n",
    "        nf = self.n_features\n",
    "        nm = self.n_mix\n",
    "\n",
    "        # Maximizing weights\n",
    "        if 'w' in self.params:\n",
    "            alphas_minus_one = self.weights_prior - 1\n",
    "            w_n = stats['post_mix_sum'] + alphas_minus_one\n",
    "            w_d = (stats['post_sum'] + alphas_minus_one.sum(axis=1))[:, None]\n",
    "            self.weights_ = w_n / w_d\n",
    "\n",
    "        # Maximizing means\n",
    "        if 'm' in self.params:\n",
    "            m_n = stats['m_n']\n",
    "            m_d = stats['post_mix_sum'] + self.means_weight\n",
    "            \n",
    "            m_d[(self.weights_ == 0) & (m_n == 0).all(axis=-1)] = 1\n",
    "            self.means_ = m_n / m_d[:, :, None]\n",
    "\n",
    "        # Maximizing covariances\n",
    "        if 'c' in self.params:\n",
    "            lambdas, mus = self.means_weight, self.means_prior\n",
    "            centered_means = self.means_ - mus\n",
    "\n",
    "            def outer_f(x):  # Outer product over features.\n",
    "                return x[..., :, None] * x[..., None, :]\n",
    "\n",
    "            if self.covariance_type == 'full':\n",
    "                centered_means_dots = outer_f(centered_means)\n",
    "\n",
    "                psis_t = np.transpose(self.covars_prior, axes=(0, 1, 3, 2))\n",
    "                nus = self.covars_weight\n",
    "\n",
    "                c_n = psis_t + lambdas[:, :, None, None] * centered_means_dots\n",
    "                c_n += stats['c_n']\n",
    "                c_d = (\n",
    "                    stats['post_mix_sum'] + 1 + nus + nf + 1\n",
    "                )[:, :, None, None]\n",
    "\n",
    "            elif self.covariance_type == 'diag':\n",
    "                alphas = self.covars_prior\n",
    "                betas = self.covars_weight\n",
    "                centered_means2 = centered_means ** 2\n",
    "\n",
    "                c_n = lambdas[:, :, None] * centered_means2 + 2 * betas\n",
    "                c_n += stats['c_n']\n",
    "                c_d = stats['post_mix_sum'][:, :, None] + 1 + 2 * (alphas + 1)\n",
    "\n",
    "            elif self.covariance_type == 'spherical':\n",
    "                centered_means_norm2 = np.einsum(  # Faster than (x**2).sum(-1)\n",
    "                    '...i,...i', centered_means, centered_means)\n",
    "\n",
    "                alphas = self.covars_prior\n",
    "                betas = self.covars_weight\n",
    "\n",
    "                c_n = lambdas * centered_means_norm2 + 2 * betas\n",
    "                c_n += stats['c_n']\n",
    "                c_d = nf * (stats['post_mix_sum'] + 1) + 2 * (alphas + 1)\n",
    "\n",
    "            elif self.covariance_type == 'tied':\n",
    "                centered_means_dots = outer_f(centered_means)\n",
    "\n",
    "                psis_t = np.transpose(self.covars_prior, axes=(0, 2, 1))\n",
    "                nus = self.covars_weight\n",
    "\n",
    "                c_n = np.einsum('ij,ijkl->ikl',\n",
    "                                lambdas, centered_means_dots) + psis_t\n",
    "                c_n += stats['c_n']\n",
    "                c_d = (stats['post_sum'] + nm + nus + nf + 1)[:, None, None]\n",
    "\n",
    "            self.covars_ = c_n / c_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gammaln, digamma\n",
    "\n",
    "from . import _utils\n",
    "\n",
    "\n",
    "def kl_dirichlet(q, p):\n",
    "\n",
    "    q = np.asarray(q)\n",
    "    p = np.asarray(p)\n",
    "    qsum = q.sum()\n",
    "    psum = p.sum()\n",
    "    return (gammaln(qsum) - gammaln(psum)\n",
    "            - np.sum(gammaln(q) - gammaln(p))\n",
    "            + np.einsum(\"i,i->\", (q - p), (digamma(q) - digamma(qsum))))\n",
    "\n",
    "\n",
    "def kl_normal_distribution(mean_q, variance_q, mean_p, variance_p):\n",
    "    result = ((np.log(variance_p / variance_q)) / 2\n",
    "              + ((mean_q - mean_p)**2 + variance_q) / (2 * variance_p)\n",
    "              - .5)\n",
    "    assert result >= 0, result\n",
    "    return result\n",
    "\n",
    "\n",
    "def kl_multivariate_normal_distribution(mean_q, covar_q, mean_p, covar_p):\n",
    "    # Ensure arrays\n",
    "    mean_q = np.asarray(mean_q)\n",
    "    covar_q = np.asarray(covar_q)\n",
    "    mean_p = np.asarray(mean_p)\n",
    "    covar_p = np.asarray(covar_p)\n",
    "\n",
    "    # Need the precision of distribution p\n",
    "    precision_p = np.linalg.inv(covar_p)\n",
    "\n",
    "    mean_diff = mean_q - mean_p\n",
    "    D = mean_q.shape[0]\n",
    "\n",
    "    # These correspond to the four terms in the ~wpenny paper documented above\n",
    "    return .5 * (_utils.logdet(covar_p) - _utils.logdet(covar_q)\n",
    "                 + np.trace(precision_p @ covar_q)\n",
    "                 + mean_diff @ precision_p @ mean_diff\n",
    "                 - D)\n",
    "\n",
    "\n",
    "def kl_gamma_distribution(b_q, c_q, b_p, c_p):\n",
    "\n",
    "    result = ((b_q - b_p) * digamma(b_q)\n",
    "              - gammaln(b_q) + gammaln(b_p)\n",
    "              + b_p * (np.log(c_q) - np.log(c_p))\n",
    "              + b_q * (c_p-c_q) / c_q)\n",
    "    assert result >= 0, result\n",
    "    return result\n",
    "\n",
    "\n",
    "def kl_wishart_distribution(dof_q, scale_q, dof_p, scale_p):\n",
    "    \n",
    "    scale_q = np.asarray(scale_q)\n",
    "    scale_p = np.asarray(scale_p)\n",
    "    D = scale_p.shape[0]\n",
    "    return ((dof_q - dof_p)/2 * _E(dof_q, scale_q)\n",
    "            - D * dof_q / 2\n",
    "            + dof_q / 2 * np.trace(scale_p @ np.linalg.inv(scale_q))\n",
    "            # Division of logarithm turned into subtraction here\n",
    "            + _logZ(dof_p, scale_p)\n",
    "            - _logZ(dof_q, scale_q))\n",
    "\n",
    "\n",
    "def _E(dof, scale):\n",
    "    r\"\"\"\n",
    "    $L(a, B) = \\int \\mathcal{Wishart}(\\Gamma; a, B) \\log |\\Gamma| d\\Gamma$\n",
    "    \"\"\"\n",
    "    return (-_utils.logdet(scale / 2)\n",
    "            + digamma((dof - np.arange(scale.shape[0])) / 2).sum())\n",
    "\n",
    "\n",
    "def _logZ(dof, scale):\n",
    "    D = scale.shape[0]\n",
    "    return ((D * (D - 1) / 4) * np.log(np.pi)\n",
    "            - dof / 2 * _utils.logdet(scale / 2)\n",
    "            + gammaln((dof - np.arange(scale.shape[0])) / 2).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
